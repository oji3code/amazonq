#==========================================================
# Test Case
#==========================================================
1. Karpenter automatically provisions nodes when pods cannot be scheduled
2. Pods are successfully scheduled on the new nodes
3. Karpenter chooses appropriate instance types based on pod requirements
4. Karpenter consolidates (removes) nodes when they're no longer needed
5. No errors in the Karpenter logs during provisioning or deprovisioning

#==========================================================
# Verify Karpenter Components Are Running
#==========================================================

# Check Karpenter pods
kubectl get pods -n karpenter

# Verify pods are in Running state
if kubectl get pods -n karpenter | grep -q "Running"; then
  echo "Karpenter is running properly"
else
  echo "Karpenter pods are not in Running state - check installation"
  exit 1
fi

# Check Karpenter logs for errors
kubectl logs -n karpenter -l app.kubernetes.io/name=karpenter -c controller | grep -i error

#==========================================================
# Create a Test Namespace
#==========================================================

cat <<EOF | kubectl apply -f -
apiVersion: v1
kind: Namespace
metadata:
  name: karpenter-test
EOF

# Verify namespace creation
kubectl get namespace karpenter-test

#==========================================================
# Create a Test Deployment
#==========================================================

cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: karpenter-test
  namespace: karpenter-test
spec:
  replicas: 5
  selector:
    matchLabels:
      app: karpenter-test
  template:
    metadata:
      labels:
        app: karpenter-test
    spec:
      containers:
      - name: karpenter-test
        image: public.ecr.aws/amazonlinux/amazonlinux:2
        command: ["sh", "-c", "while true; do echo Hello Karpenter; sleep 60; done"]
        resources:
          requests:
            cpu: "1"
            memory: "1Gi"
          limits:
            cpu: "1"
            memory: "1Gi"
      # Add tolerations to allow scheduling on nodes with Karpenter's disruption taint
      tolerations:
      - key: "karpenter.sh/disruption"
        operator: "Exists"
EOF

# Verify deployment creation
kubectl get deployment -n karpenter-test karpenter-test

# Check if pods are pending (which should trigger Karpenter)
kubectl get pods -n karpenter-test

#==========================================================
# Monitor Node Provisioning
#==========================================================

# Watch for new nodes being created
kubectl get nodes -w

kubectl logs -f -n karpenter -l app.kubernetes.io/name=karpenter -c controller

timeout 300 bash -c 'until kubectl get nodes -l karpenter.sh/provisioner-name | grep -q "Ready"; do echo "Waiting for nodes to be provisioned..."; sleep 10; done'

#==========================================================
# Verify Pod Scheduling
#==========================================================

# Get pods and their node assignments
kubectl get pods -n karpenter-test -o wide

# Check the status of your deployment
kubectl get deployment -n karpenter-test karpenter-test

# Wait for pods to be scheduled (timeout after 5 minutes)
timeout 300 bash -c 'until kubectl get pods -n karpenter-test | grep -q "Running"; do echo "Waiting for pods to start..."; sleep 10; done'

# Verify all pods are running
if kubectl get pods -n karpenter-test | grep -v "Running" | grep -q "karpenter-test"; then
  echo "Some pods are not in Running state - check Karpenter logs"
else
  echo "All pods are running successfully"
fi


#==========================================================
# Test Different Instance Types
#==========================================================

cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: karpenter-test-large
  namespace: karpenter-test
spec:
  replicas: 2
  selector:
    matchLabels:
      app: karpenter-test-large
  template:
    metadata:
      labels:
        app: karpenter-test-large
    spec:
      containers:
      - name: karpenter-test
        image: public.ecr.aws/amazonlinux/amazonlinux:2
        command: ["sh", "-c", "while true; do echo Hello Karpenter; sleep 60; done"]
        resources:
          requests:
            cpu: "4"
            memory: "8Gi"
          limits:
            cpu: "4"
            memory: "8Gi"
      tolerations:
      - key: "karpenter.sh/disruption"
        operator: "Exists"
EOF

# Verify deployment creation
kubectl get deployment -n karpenter-test karpenter-test-large

# Monitor for new node provisioning
kubectl get nodes -w

timeout 300 bash -c 'until kubectl get pods -n karpenter-test -l app=karpenter-test-large | grep -q "Running"; do echo "Waiting for large pods to start..."; sleep 10; done'


#==========================================================
# Test Node Consolidation
#==========================================================

Scale down the deployments to trigger node consolidation:

cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: karpenter-test
  namespace: karpenter-test
spec:
  replicas: 0
  selector:
    matchLabels:
      app: karpenter-test
  template:
    metadata:
      labels:
        app: karpenter-test
    spec:
      containers:
      - name: karpenter-test
        image: public.ecr.aws/amazonlinux/amazonlinux:2
        command: ["sh", "-c", "while true; do echo Hello Karpenter; sleep 60; done"]
        resources:
          requests:
            cpu: "1"
            memory: "1Gi"
          limits:
            cpu: "1"
            memory: "1Gi"
EOF

cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: karpenter-test-large
  namespace: karpenter-test
spec:
  replicas: 0
  selector:
    matchLabels:
      app: karpenter-test-large
  template:
    metadata:
      labels:
        app: karpenter-test-large
    spec:
      containers:
      - name: karpenter-test
        image: public.ecr.aws/amazonlinux/amazonlinux:2
        command: ["sh", "-c", "while true; do echo Hello Karpenter; sleep 60; done"]
        resources:
          requests:
            cpu: "4"
            memory: "8Gi"
          limits:
            cpu: "4"
            memory: "8Gi"
EOF

# Verify deployments scaled to 0
kubectl get deployment -n karpenter-test

# Watch nodes
kubectl get nodes -w

# Note: Node consolidation depends on your NodePool's consolidationPolicy and consolidateAfter settings
# If consolidationPolicy is set to "WhenUnderutilized" and consolidateAfter is set to a short duration,
# nodes should be removed within a few minutes

kubectl logs -f -n karpenter -l app.kubernetes.io/name=karpenter -c controller | grep -i "consolidat"

#==========================================================
# Test Multi-Architecture Support (If your NodePool supports multiple architectures, test with ARM-based workloads:)
#==========================================================

cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: karpenter-test-arm
  namespace: karpenter-test
spec:
  replicas: 2
  selector:
    matchLabels:
      app: karpenter-test-arm
  template:
    metadata:
      labels:
        app: karpenter-test-arm
    spec:
      containers:
      - name: karpenter-test
        image: public.ecr.aws/amazonlinux/amazonlinux:2
        command: ["sh", "-c", "while true; do echo Hello Karpenter; sleep 60; done"]
        resources:
          requests:
            cpu: "1"
            memory: "1Gi"
          limits:
            cpu: "1"
            memory: "1Gi"
      nodeSelector:
        kubernetes.io/arch: arm64
      tolerations:
      - key: "karpenter.sh/disruption"
        operator: "Exists"
EOF

# Monitor for ARM node provisioning
kubectl get nodes -l kubernetes.io/arch=arm64

#==========================================================
# Test Scaling with Load(Create a horizontal pod autoscaler)
#==========================================================

cat <<EOF | kubectl apply -f -
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: karpenter-test-hpa
  namespace: karpenter-test
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: karpenter-test
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 50
EOF

# Verify HPA creation
kubectl get hpa -n karpenter-test

Create a test deployment with 1 replica:

cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: karpenter-test
  namespace: karpenter-test
spec:
  replicas: 1
  selector:
    matchLabels:
      app: karpenter-test
  template:
    metadata:
      labels:
        app: karpenter-test
    spec:
      containers:
      - name: karpenter-test
        image: public.ecr.aws/amazonlinux/amazonlinux:2
        command: ["sh", "-c", "while true; do echo Hello Karpenter; sleep 60; done"]
        resources:
          requests:
            cpu: "500m"
            memory: "500Mi"
          limits:
            cpu: "500m"
            memory: "500Mi"
      tolerations:
      - key: "karpenter.sh/disruption"
        operator: "Exists"
EOF

Create an effective load generator:

cat <<EOF | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata:
  name: load-generator
  namespace: karpenter-test
spec:
  replicas: 2
  selector:
    matchLabels:
      app: load-generator
  template:
    metadata:
      labels:
        app: load-generator
    spec:
      containers:
      - name: load-generator
        image: busybox
        command: 
        - /bin/sh
        - -c
        - "while true; do yes > /dev/null; done"
        resources:
          requests:
            cpu: "100m"
            memory: "100Mi"
          limits:
            cpu: "100m"
            memory: "100Mi"
EOF

Monitor HPA and scaling:

# Watch HPA metrics
kubectl get hpa -n karpenter-test -w

# Monitor pod scaling
kubectl get pods -n karpenter-test -l app=karpenter-test -w

# Monitor node provisioning
kubectl get nodes -w

#==========================================================
# Analyze Karpenter Events
#==========================================================

# Get Karpenter events
kubectl get events --sort-by='.lastTimestamp' | grep -i karpenter

# Check NodePool status
kubectl describe nodepool

# Check EC2NodeClass status
kubectl describe ec2nodeclass

#==========================================================
# Clean Up Test Resources
#==========================================================

# Delete the namespace and all resources in it
kubectl delete namespace karpenter-test

# Verify namespace deletion
kubectl get namespace karpenter-test

# Verify no Karpenter-provisioned nodes remain (may take a few minutes)
kubectl get nodes -l karpenter.sh/provisioner-name

# If nodes remain, check Karpenter logs
kubectl logs -n karpenter -l app.kubernetes.io/name=karpenter -c controller | grep -i "terminat"

#==========================================================
# Troubleshooting(If tests fail, use these commands to troubleshoot:)
#==========================================================

# Check Karpenter logs
kubectl logs -n karpenter -l app.kubernetes.io/name=karpenter -c controller

# Check pod events
kubectl describe pods -n karpenter-test -l app=karpenter-test

# Check NodePool status
kubectl describe nodepool

# Check EC2NodeClass status
kubectl describe ec2nodeclass

# Check node conditions
kubectl describe nodes -l karpenter.sh/provisioner-name

# Check AWS API calls (if you have CloudTrail enabled)
aws cloudtrail lookup-events --lookup-attributes AttributeKey=EventName,AttributeValue=RunInstances
